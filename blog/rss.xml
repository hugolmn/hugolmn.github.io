<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Hugo Le Moine Blog</title>
        <link>https://hugolmn.github.io/blog</link>
        <description>Hugo Le Moine Blog</description>
        <lastBuildDate>Sat, 11 Jun 2022 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Twitter API - Delete my tweets!]]></title>
            <link>https://hugolmn.github.io/blog/2022/06/11/delete-my-tweets</link>
            <guid>https://hugolmn.github.io/blog/2022/06/11/delete-my-tweets</guid>
            <pubDate>Sat, 11 Jun 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[I reset my account using twitter API, to delete all my existing tweets.]]></description>
            <content:encoded><![CDATA[<p>I opened my twitter account <a href="https://twitter.com/hugo_le_moine_">@hugo_le_moine_</a> back in 2012 during my high school years, and have mostly used it to retweet/like content, using it like a bookmarking tool. Over time, I accumulated a couple hundreds tweets and I certainly don't remember &gt;95% of them.</p>
<p>It's now time for a deep cleaning!</p>
<p>But... Twitter does not let you delete more than a tweet at time. Then how to automate this ?</p>
<hr>
<h1 id="twitter-api">Twitter API</h1>
<p>Leveraging <a href="https://developer.twitter.com/en/docs/api-reference-index">Twitter API</a> is the first option I explored. Twitter has built quite a comprehensive set of endpoints enabling developer to:</p>
<ul>
<li>
<p>Post and delete a tweet / retweet / like / quote / bookmark</p>
</li>
<li>
<p>Search tweets</p>
</li>
<li>
<p>Follow / unfollow</p>
</li>
</ul>
<h5 id="applying-for-a-developer-account">Applying for a developer account</h5>
<p>Using Twitter API requires a <a href="https://developer.twitter.com/en">developer account</a>. The "Essential" level was enough for this tiny project. As I plan to try other functionalities later on, I got the "Elevated" level.</p>
<h5 id="creating-an-app-and-getting-your-keys-and-tokens">Creating an app and getting your keys and tokens</h5>
<p>Before we can dive into the API and play with the endpoints, we need some more configuration: <a href="https://developer.twitter.com/en/portal/apps/new">creating an app</a>.</p>
<blockquote>
<p>Note: keys generated cannot be displayed again, in case you forgot to write them somewhere, you can still regenerate them.</p>
</blockquote>
<p>In the "Settings" tab, the "OAuth 1.0a" must be activated, with "Read and write" permissions. The callback URI / Redirect URL can be your own github page.</p>
<p>In the Keys and tokens tab, you can now get your consumer keys and generate authentication tokens for your personal account.</p>
<hr>
<h1 id="tweepy">Tweepy</h1>
<p>APIs can be used in most programming languages, and as Python is the one I am most comfortable with, I looked for a library that wraps the Twitter API for conveniency. I came across <a href="https://github.com/tweepy/tweepy">tweepy</a>.</p>
<h6 id="credentials">Credentials</h6>
<p>Your credentials can be stored in a yaml file to avoid hard-coding them in your python files (<strong>do not commit your keys!</strong>). For example in <code>delete-my-tweets.yaml</code>:</p>
<pre><code class="language-yaml">delete-my-tweets-dev:
  consumer_key: ***********
  consumer_secret: **************
  access_token: *************************
  access_token_secret: **********************
  bearer_token: ********************************************
</code></pre>
<p>Then, you can load them using <a href="https://github.com/yaml/pyyaml">pyyaml</a>:</p>
<pre><code class="language-python">import yaml

# https://stackoverflow.com/a/1774043/13765085
with open("delete-my-tweets.yaml", "r") as stream:
    try:
        parameters = yaml.safe_load(stream)['delete-my-tweets-dev']
    except yaml.YAMLError as exc:
        print(exc)
</code></pre>
<h5 id="initialize-client">Initialize client</h5>
<pre><code class="language-python">import tweepy

client = tweepy.Client(
    bearer_token=parameters['bearer_token'],
    consumer_key=parameters['consumer_key'],
    consumer_secret=parameters['consumer_secret'],
    access_token=parameters['access_token'],
    access_token_secret=parameters['access_token_secret'],
    wait_on_rate_limit=True, # to automatically wait when rate is exceeded
)
</code></pre>
<h5 id="retrieve-users-tweets">Retrieve user's tweets</h5>
<p>To get the list of tweets from a user, it is required to provide the id associated with the account. To get this information, you can use the method <code>get_user</code> and provide your username.</p>
<pre><code class="language-python">user = client.get_user(username='hugo_le_moine_')
</code></pre>
<p>We can now retrieve the account's tweets. The method <code>get_user_tweets</code> allows to collect tweets 100 at a time using pagination: each call will return a maximum of <code>max_results</code> tweets, and provide a token <code>next_token</code> allowing to collect the next tweets by passing it as an argument to the next call.</p>
<blockquote>
<p>This method only allows to retrieve the most recent 3200 tweets.</p>
</blockquote>
<p>We can write a recursive function that will go dig until the no more <code>next_token</code> is provided, and return all the tweets as a list.</p>
<pre><code class="language-python">def collect_tweet_ids(next_token=None):
    # timeline contains tweets and metadata about the user
    timeline = client.get_users_tweets(
        id=user.data.id,
        max_results=100,
        pagination_token=next_token
    )

    # get list of tweets
    tweet_list= timeline.data

    # if a next page exists, append its results to the list of tweets
    if 'next_token' in timeline.meta:
        tweet_list += collect_tweet_ids(timeline.meta['next_token'])
    return tweet_list
</code></pre>
<p>We can now collect the tweets history:</p>
<pre><code class="language-python">tweets = collect_tweet_ids()
</code></pre>
<h5 id="delete-tweets">Delete tweets</h5>
<p>Finally, we can use the tweet ids to delete them using the <code>delete_tweet</code> method.</p>
<blockquote>
<p><strong>Warning</strong>: twitter cap rate limits tweet deletion to 50 per 15 min window. Deleting all tweets can take a while, more than 8 hours in my case.</p>
</blockquote>
<p>I use <a href="https://github.com/tqdm/tqdm">tqdm</a> to display a progress bar.</p>
<pre><code class="language-python">from tqdm import tqdm

for tweet in tqdm(tweets):
    client.delete_tweet(tweet.id)
</code></pre>
<hr>
<p>That's it!! I can now resume using twitter with a seemingly blank account, except that I get to keep all my settings and followings. I learnt to use the Twitter API and it's python wrapper tweepy, not in depth, but I will quite likely get back to it in the future!</p>
<p>Has it been useful to you? Any faster way that you found? Any feedback will be appreciated, hit me up on twitter <a href="https://twitter.com/hugo_le_moine_">@hugo_le_moine_</a>!</p>]]></content:encoded>
            <category>Python</category>
            <category>API</category>
            <category>Twitter</category>
        </item>
        <item>
            <title><![CDATA[SNCF Open Data: found items - 2]]></title>
            <link>https://hugolmn.github.io/blog/2020/07/28/sncf-found-items-2</link>
            <guid>https://hugolmn.github.io/blog/2020/07/28/sncf-found-items-2</guid>
            <pubDate>Tue, 28 Jul 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Data visualization using plotly]]></description>
            <content:encoded><![CDATA[<h2 id="map-of-found-items">Map of found items</h2>
<p>Following a previous <a href="https://hugolmn.github.io/2020/06/27/sncf-found-items.html">article</a> analyzing monthly evolution as well as concentration of found items, here is another perspective on these data made available by the SNCF. Obviously, the higher the number of passengers, the higher the number of found items. Now, let's look into the details!</p>
<p>On this map are represented the number of found items per 100k passengers between 2015 and 2018. Only stations with more than 10 found items are shown.</p>
<div class="plotly-container"><iframe width="100%" height="700" frameborder="0" scrolling="no" src="/graphs/sncf-found-items-map.html"></iframe></div>
<h4 id="what-can-we-conclude-from-this-visualization-">What can we conclude from this visualization ?</h4>
<p>Clearly, there is a significant difference between the area of Paris/north of France, and anywhere else in the country. Nonetheless, one should avoid easy conclusion: are people more careful about their belongings or less prone to hand found items back ? The two options are possibly and even likely connected, but additional data are required to investigate. Perhaps in another part :)</p>
<h2 id="data">Data</h2>
<ul>
<li><a href="https://ressources.data.sncf.com/explore/dataset/frequentation-gares">SNCF : train station attendance</a></li>
<li><a href="https://ressources.data.sncf.com/explore/dataset/referentiel-gares-voyageurs">SNCF: train station list</a></li>
<li><a href="https://ressources.data.sncf.com/explore/dataset/objets-trouves-restitution">SNCF: found items</a></li>
</ul>
<h2 id="libraries">Libraries</h2>
<p>The following libraries are imported:</p>
<ul>
<li><a href="https://pandas.pydata.org/">pandas</a> and <a href="https://numpy.org/">numpy</a> for data processing</li>
<li><a href="https://plotly.com/python/">plotly</a>.colors to use a specific colorscale</li>
<li><a href="https://plotly.com/python/">plotly</a>.graph_object for data visualization</li>
</ul>
<pre><code class="language-python">import pandas as pd						
import numpy as np						
import plotly.colors
import plotly.graph_objects as go
</code></pre>
<h2 id="processing">Processing</h2>
<h4 id="1-reading-csv-files">1. Reading csv files</h4>
<pre><code class="language-python">df_frequentation = pd.read_csv('data/frequentation-gares.csv', sep=';')
df_gares = pd.read_csv('data/referentiel-gares-voyageurs.csv', sep=';')
df_items = pd.read_csv('data/objets-trouves-restitution.csv', sep=';')
</code></pre>
<p>These two dataframes contain contain the following data:</p>
<ul>
<li>
<p><code>df_frequentation</code>: train station attendance data</p>
</li>
<li>
<p><code>df_gares</code>: train station data, including latitude and longitude.</p>
</li>
<li>
<p><code>df_items</code>: found item data, including date and location.</p>
</li>
</ul>
<p>Sample data from <code>df_frequentation</code></p>
<table><thead><tr><th></th><th>Nom de la gare</th><th>Code UIC complet</th><th>Code postal</th><th>Segmentation DRG 2018</th><th>Total Voyageurs 2018</th><th>Total Voyageurs + Non voyageurs 2018</th><th>Total Voyageurs 2017</th><th>Total Voyageurs + Non voyageurs 2017</th><th>Total Voyageurs 2016</th><th>Total Voyageurs + Non voyageurs 2016</th><th>Total Voyageurs 2015</th><th>Total Voyageurs + Non voyageurs 2015</th></tr></thead><tbody><tr><td>0</td><td>Abancourt</td><td>87313759</td><td>60220</td><td>c</td><td>40228</td><td>40228</td><td>43760</td><td>43760</td><td>41096</td><td>41096.551614</td><td>39720</td><td>39720</td></tr><tr><td>1</td><td>Agay</td><td>87757559</td><td>83530</td><td>c</td><td>15093</td><td>15093</td><td>14154</td><td>14154</td><td>19240</td><td>19240.514370</td><td>19121</td><td>19121</td></tr><tr><td>2</td><td>Agde</td><td>87781278</td><td>34300</td><td>a</td><td>588297</td><td>735372</td><td>697091</td><td>871364</td><td>660656</td><td>825820.929253</td><td>662516</td><td>828146</td></tr><tr><td>3</td><td>Agonac</td><td>87595157</td><td>24460</td><td>c</td><td>1492</td><td>1492</td><td>1583</td><td>1583</td><td>1134</td><td>1134.699996</td><td>1127</td><td>1127</td></tr><tr><td>4</td><td>Aigrefeuille Le Thou</td><td>87485193</td><td>17290</td><td>c</td><td>18670</td><td>18670</td><td>14513</td><td>14513</td><td>266</td><td>266.157144</td><td>0</td><td>0</td></tr></tbody></table>
<p>Sample data from <code>df_gares</code></p>
<table><thead><tr><th></th><th>Code plate-forme</th><th>Intitulé gare</th><th>Intitulé fronton de gare</th><th>Gare DRG</th><th>Gare étrangère</th><th>Agence gare</th><th>Région SNCF</th><th>Unité gare</th><th>UT</th><th>Nbre plateformes</th><th>...</th><th>Longitude WGS84</th><th>Latitude WGS84</th><th>Code UIC</th><th>TVS</th><th>Segment DRG</th><th>Niveau de service</th><th>SOP</th><th>RG</th><th>Date fin validité plateforme</th><th>WGS 84</th></tr></thead><tbody><tr><td>0</td><td>00007-1</td><td>Bourg-Madame</td><td>Bourg-Madame</td><td>True</td><td>False</td><td>Agence Grand Sud</td><td>REGION LANGUEDOC-ROUSSILLON</td><td>UG Languedoc Roussillon</td><td>BOURG MADAME GARE</td><td>1</td><td>...</td><td>1.948670</td><td>42.432407</td><td>87784876</td><td>BMD</td><td>c</td><td>1.0</td><td>NaN</td><td>GARES C LANGUEDOC ROUSSILLON</td><td>NaN</td><td>42.4324069,1.9486704</td></tr><tr><td>1</td><td>00014-1</td><td>Bolquère - Eyne</td><td>Bolquère - Eyne</td><td>True</td><td>False</td><td>Agence Grand Sud</td><td>REGION LANGUEDOC-ROUSSILLON</td><td>UG Languedoc Roussillon</td><td>BOLQUERE EYNE GARE</td><td>1</td><td>...</td><td>2.087559</td><td>42.497873</td><td>87784801</td><td>BQE</td><td>c</td><td>1.0</td><td>NaN</td><td>GARES C LANGUEDOC ROUSSILLON</td><td>NaN</td><td>42.4978734,2.0875591</td></tr><tr><td>2</td><td>00015-1</td><td>Mont-Louis - La Cabanasse</td><td>Mont-Louis - La Cabanasse</td><td>True</td><td>False</td><td>Agence Grand Sud</td><td>REGION LANGUEDOC-ROUSSILLON</td><td>UG Languedoc Roussillon</td><td>MONT LOUIS LA CABANASSE GARE</td><td>1</td><td>...</td><td>2.113138</td><td>42.502090</td><td>87784793</td><td>MTC</td><td>c</td><td>1.0</td><td>NaN</td><td>GARES C LANGUEDOC ROUSSILLON</td><td>NaN</td><td>42.5020902,2.1131379</td></tr></tbody></table>
<p>Sampel data from <code>df_items</code></p>
<table><thead><tr><th></th><th>Date</th><th>Date et heure de restitution</th><th>Gare</th><th>Code UIC</th><th>Nature d'objets</th><th>Type d'objets</th><th>Type d'enregistrement</th></tr></thead><tbody><tr><td>0</td><td>2014-03-09T14:25:29+01:00</td><td>NaN</td><td>Paris Montparnasse</td><td>87391003.0</td><td>Manteau, veste, blazer, parka, blouson, cape</td><td>Vêtements, chaussures</td><td>Objet trouvé</td></tr><tr><td>1</td><td>2018-01-23T15:07:32+01:00</td><td>NaN</td><td>Saint-Étienne Châteaucreux</td><td>87726000.0</td><td>Montre</td><td>Bijoux, montres</td><td>Objet trouvé</td></tr><tr><td>2</td><td>2018-02-06T15:35:49+01:00</td><td>NaN</td><td>Rennes</td><td>87471003.0</td><td>Clés, porte-clés</td><td>Clés, porte-clés, badge magnétique</td><td>Objet trouvé</td></tr></tbody></table>
<h4 id="2-merging-dataframes">2. Merging dataframes</h4>
<p>The three dataframes have a column identifier: the UIC code ("Union Internationale des Chemins de fer", International Union of Railways in french). We can merge them based on this unique value for each station.</p>
<pre><code class="language-python">df = df_gares.merge(
    right=df_frequentation, 
    left_on='Code UIC', 
    right_on='Code UIC complet', 
    how='inner')

df = df.merge(df_items, on='Code UIC', how='right')
</code></pre>
<h4 id="3-conversion-to-datetime">3. Conversion to datetime</h4>
<p>For now, the DataFrame has a Date column, but was not assigned any particular format.</p>
<pre><code class="language-python">df.Date.dtype
</code></pre>
<blockquote>
<pre><code>&gt;&gt; dtype('O')
</code></pre>
</blockquote>
<p>In order to have a standardized datetime, <code>pandas.to_datetime</code> can be applied to get a datetime, and then <code>tz_convert</code> to make it french time.</p>
<pre><code class="language-python">df['Date'] = pd.to_datetime(df.Date, utc=True).dt.tz_convert('Europe/Paris')
</code></pre>
<h4 id="4-filtering-dates">4. Filtering dates</h4>
<p>As I want to analyze the number of found items according to the number of passengers, I must filter out data which is not in the period 2015-2018 as these are the only years for which the number of passengers is available.</p>
<pre><code class="language-python">df = df.loc[(df.Date.dt.year &gt;= 2015) &amp; (df.Date.dt.year &lt;= 2018)]
</code></pre>
<h4 id="5-total-number-of-passengers-per-station">5. Total number of passengers per station</h4>
<pre><code class="language-python">df['Passengers'] =(df['Total Voyageurs 2018'] 
                            + df['Total Voyageurs 2017'] 
                            + df['Total Voyageurs 2016'] 
                            + df['Total Voyageurs 2015'])
</code></pre>
<h4 id="6-grouping-data-by-train-station">6. Grouping data by train station</h4>
<p>Data is grouped by station name, geographical coordinates are kept, as well as the count of found items, and the number of passengers.</p>
<pre><code class="language-python">df = (df.groupby(by="Intitulé gare")
        .agg({'Longitude WGS84': 'first',
              'Latitude WGS84': 'first',
              'Nature d\'objets': len,
              'Passengers': 'first'})
        .rename(columns={'Nature d\'objets': 'item_count'})
)
</code></pre>
<h4 id="7-count-per-100k">7. Count per 100k</h4>
<p>In this blog post, the focus is put on the ratio item/pax. A straightforward division would lead to number hard to imagine. Therefore, using a ratio per 100k passengers is more interesting. I restricted the selection to stations having at least 10 found items .</p>
<pre><code class="language-python">df['count_per_100kpax'] = df.item_count / df.Passengers * 1e5
df = df.sort_values(by='count_per_100kpax', ascending=False)
df = df[df.item_count &gt;= 10]
</code></pre>
<h4 id="8-creation-of-categories">8. Creation of categories</h4>
<p>To group data by category on the map, we need to bin them. I chose a standard quantile binning with 20% of data in each bin.</p>
<pre><code class="language-python">df['group'] = pd.qcut(df['count_per_100kpax'], q=np.linspace(0, 1, 6))
</code></pre>
<h4 id="9-what-we-have-so-far">9. What we have so far</h4>
<table><thead><tr><th>Intitulé gare</th><th>Longitude WGS84</th><th>Latitude WGS84</th><th>item_count</th><th>Passengers</th><th>count_per_100kpax</th><th>category</th></tr></thead><tbody><tr><td>Versailles Rive Droite</td><td>2.134752</td><td>48.809653</td><td>39</td><td>21309519.0</td><td>0.183017</td><td>(0.182, 3.623]</td></tr><tr><td>Poissy</td><td>2.041368</td><td>48.932901</td><td>80</td><td>43120958.0</td><td>0.185525</td><td>(0.182, 3.623]</td></tr><tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><td>Hendaye</td><td>-1.781724</td><td>43.353132</td><td>2873</td><td>1422622.0</td><td>201.951045</td><td>(24.664, 323.36]</td></tr><tr><td>Le Croisic</td><td>-2.507442</td><td>47.289836</td><td>1358</td><td>419966.0</td><td>323.359510</td><td>(24.664, 323.36]</td></tr></tbody></table>
<h2 id="visualization">Visualization</h2>
<p>Now that all the processing part is completed, we can proceed to the  visualization.</p>
<pre><code class="language-python">fig = go.Figure()
colors = plotly.colors.sequential.Inferno # Custom colormap

# One scattermapbox per group
for i, group in enumerate(df.group.cat.categories):
    df_sub = df[df.group == group]
    fig.add_trace(go.Scattermapbox(
            lat=df_sub['Latitude WGS84'], 
            lon=df_sub['Longitude WGS84'],
            text=df_sub.index,
            marker=dict(
                color=colors[2*i],
                size=df_sub['count_per_100kpax'],
                sizemin=3,
                sizeref=.35,
                sizemode='area',
                opacity=.8,
            ),
            meta=df_sub['item_count'],
            hovertemplate="%{text}" + "&lt;br&gt;" 
                            + "Found items: %{meta}" + "&lt;br&gt;" 
                            + "Per 100kPax: " + "%{marker.size:.1f}",
            name=f'&gt; {cat.left:.0f} per 100kPax',          
    ))
</code></pre>
<pre><code class="language-python"># Defining map style, margins, and original position
fig.update_layout(
    mapbox_style="open-street-map",
    #title='Passengers per french train station in 2018',
    margin={'l': 0, 'r': 0, 't': 0, 'b': 0},
    mapbox=dict(
        center={'lon': 2.39, 'lat': 47.09},
        zoom=4
    ),
)
# Legend layout
fig.update_layout(legend={'orientation': 'h', 'y': 0})
</code></pre>
<p>Link to the <a href="https://github.com/hugolmn/dataviz/blob/master/SNCF_found_item_2.ipynb">Jupyter notebook</a>.</p>]]></content:encoded>
            <category>Data Visualization</category>
            <category>Python</category>
            <category>plotly</category>
        </item>
        <item>
            <title><![CDATA[Covid19 and Google Trends]]></title>
            <link>https://hugolmn.github.io/blog/2020/07/08/usa-covid-google-trends</link>
            <guid>https://hugolmn.github.io/blog/2020/07/08/usa-covid-google-trends</guid>
            <pubDate>Wed, 08 Jul 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Trend anaysis between covid19 cases and Google searches]]></description>
            <content:encoded><![CDATA[<h2 id="usa-covid19-cases-vs-google-trends">USA Covid19 Cases vs. Google Trends</h2>
<p>Based on user searches, <a href="https://www.google.org/flutrends/about/">Google Flu</a> tried to estimate flu prevalence among populations. In this simple visualization I try to use a similar, yet simplified approach. Below are compared USA reported covid19 cases against Google Trends popularity for "Coronavirus" and "Worldometer" (which is a website used worldwide to track this pandemic).</p>
<iframe width="100%" height="400" frameborder="0" scrolling="no" src="//plotly.com/~hugolmn/55.embed?link=false&amp;autosize=true&amp;modebar=false"></iframe>
<h2 id="timeline">Timeline</h2>
<ul>
<li>January 2020: quick spread in China</li>
<li>Mid-February : spread to Europe</li>
<li>"Coronavirus" popularity peaks on 15th March, when USA cases growth rate is maximal.</li>
<li>"Worldometer" popularity peaks two weeks later.</li>
<li>11th April: USA 7-day moving average peaks.</li>
<li>31th May: USA 7-day moving average bottoms to a 2-month low.</li>
<li>Since June: cases are up to new daily records, but popularity of "Coronavirus" remains low.</li>
</ul>
<h2 id="data">Data</h2>
<ul>
<li><a href="https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/cases-in-us.html">CDC USA</a></li>
<li><a href="https://trends.google.com/trends/explore?date=2020-01-01%202020-12-31&amp;geo=US&amp;q=%2Fm%2F01cpyy">Google Trends</a></li>
</ul>
<p>Link to the <a href="https://github.com/hugolmn/dataviz/blob/master/USA-Google-Trends.ipynb">Jupyter notebook</a>.</p>]]></content:encoded>
            <category>Data Visualization</category>
            <category>Python</category>
            <category>plotly</category>
        </item>
        <item>
            <title><![CDATA[SNCF Open Data: found items - 1]]></title>
            <link>https://hugolmn.github.io/blog/2020/06/27/sncf-found-items</link>
            <guid>https://hugolmn.github.io/blog/2020/06/27/sncf-found-items</guid>
            <pubDate>Sat, 27 Jun 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Data visualization using plotly]]></description>
            <content:encoded><![CDATA[<h2 id="monthly-found-items-in-french-train-stations">Monthly found items in french train stations</h2>
<p>We can view found items as a proxy for passenger traffic. On this first chart, we can clearly identify a few traffic disruptions:</p>
<ul>
<li>April 2018 to June 2018 : discontinuous strike against project to reform SNCF.</li>
<li>December 2019 : strike against pension reform.</li>
<li>March 2020 to June 2020 : Covid-19 restrictions.</li>
</ul>
<p>We can also observe a peak in July each year, corresponding to summer vacations for most people.</p>
<iframe height="400" width="100%" frameborder="0" scrolling="no" src="//plotly.com/~hugolmn/21.embed?link=false&amp;autosize=true"></iframe>
<h2 id="18-of-stations-represent-80-of-found-items">18% of stations represent 80% of found items</h2>
<iframe height="600" width="100%" frameborder="0" scrolling="no" src="//plotly.com/~hugolmn/14.embed?link=false&amp;autosize=true"></iframe>
<h2 id="data">Data</h2>
<ul>
<li><a href="https://ressources.data.sncf.com/explore/dataset/referentiel-gares-voyageurs">SNCF: train station list</a></li>
<li><a href="https://ressources.data.sncf.com/explore/dataset/objets-trouves-restitution">SNCF: found items</a></li>
</ul>
<h2 id="libraries">Libraries</h2>
<p>The following libraries are imported:</p>
<ul>
<li><a href="https://pandas.pydata.org/">pandas</a> and <a href="https://numpy.org/">numpy</a> for data processing</li>
<li><a href="https://plotly.com/python/">plotly</a>.colors to use a specific colorscale</li>
<li><a href="https://plotly.com/python/">plotly</a>.graph_object for data visualization</li>
</ul>
<pre><code class="language-python">import pandas as pd						
import numpy as np						
import plotly.colors
import plotly.graph_objects as go
</code></pre>
<h2 id="processing">Processing</h2>
<h4 id="1-reading-csv-files">1. Reading csv files</h4>
<pre><code class="language-python">df_gares = pd.read_csv('data/referentiel-gares-voyageurs.csv', sep=';')
df_items = pd.read_csv('data/objets-trouves-restitution.csv', sep=';')
</code></pre>
<p>These two dataframes contain contain the following data:</p>
<ul>
<li><code>df_gares</code>: train station data, including latitude and longitude.</li>
<li><code>df_items</code>: found item data, including date and location.</li>
</ul>
<p>Sample data from <code>df_gares</code></p>
<table><thead><tr><th></th><th>Code plate-forme</th><th>Intitulé gare</th><th>Intitulé fronton de gare</th><th>Gare DRG</th><th>Gare étrangère</th><th>Agence gare</th><th>Région SNCF</th><th>Unité gare</th><th>UT</th><th>Nbre plateformes</th><th>...</th><th>Longitude WGS84</th><th>Latitude WGS84</th><th>Code UIC</th><th>TVS</th><th>Segment DRG</th><th>Niveau de service</th><th>SOP</th><th>RG</th><th>Date fin validité plateforme</th><th>WGS 84</th></tr></thead><tbody><tr><td>0</td><td>00007-1</td><td>Bourg-Madame</td><td>Bourg-Madame</td><td>True</td><td>False</td><td>Agence Grand Sud</td><td>REGION LANGUEDOC-ROUSSILLON</td><td>UG Languedoc Roussillon</td><td>BOURG MADAME GARE</td><td>1</td><td>...</td><td>1.948670</td><td>42.432407</td><td>87784876</td><td>BMD</td><td>c</td><td>1.0</td><td>NaN</td><td>GARES C LANGUEDOC ROUSSILLON</td><td>NaN</td><td>42.4324069,1.9486704</td></tr><tr><td>1</td><td>00014-1</td><td>Bolquère - Eyne</td><td>Bolquère - Eyne</td><td>True</td><td>False</td><td>Agence Grand Sud</td><td>REGION LANGUEDOC-ROUSSILLON</td><td>UG Languedoc Roussillon</td><td>BOLQUERE EYNE GARE</td><td>1</td><td>...</td><td>2.087559</td><td>42.497873</td><td>87784801</td><td>BQE</td><td>c</td><td>1.0</td><td>NaN</td><td>GARES C LANGUEDOC ROUSSILLON</td><td>NaN</td><td>42.4978734,2.0875591</td></tr><tr><td>2</td><td>00015-1</td><td>Mont-Louis - La Cabanasse</td><td>Mont-Louis - La Cabanasse</td><td>True</td><td>False</td><td>Agence Grand Sud</td><td>REGION LANGUEDOC-ROUSSILLON</td><td>UG Languedoc Roussillon</td><td>MONT LOUIS LA CABANASSE GARE</td><td>1</td><td>...</td><td>2.113138</td><td>42.502090</td><td>87784793</td><td>MTC</td><td>c</td><td>1.0</td><td>NaN</td><td>GARES C LANGUEDOC ROUSSILLON</td><td>NaN</td><td>42.5020902,2.1131379</td></tr></tbody></table>
<p>Sampel data from <code>df_items</code></p>
<table><thead><tr><th></th><th>Date</th><th>Date et heure de restitution</th><th>Gare</th><th>Code UIC</th><th>Nature d'objets</th><th>Type d'objets</th><th>Type d'enregistrement</th></tr></thead><tbody><tr><td>0</td><td>2014-03-09T14:25:29+01:00</td><td>NaN</td><td>Paris Montparnasse</td><td>87391003.0</td><td>Manteau, veste, blazer, parka, blouson, cape</td><td>Vêtements, chaussures</td><td>Objet trouvé</td></tr><tr><td>1</td><td>2018-01-23T15:07:32+01:00</td><td>NaN</td><td>Saint-Étienne Châteaucreux</td><td>87726000.0</td><td>Montre</td><td>Bijoux, montres</td><td>Objet trouvé</td></tr><tr><td>2</td><td>2018-02-06T15:35:49+01:00</td><td>NaN</td><td>Rennes</td><td>87471003.0</td><td>Clés, porte-clés</td><td>Clés, porte-clés, badge magnétique</td><td>Objet trouvé</td></tr></tbody></table>
<h4 id="2-conversion-to-datetime">2. Conversion to datetime</h4>
<p>For now, the DataFrame has a Date column, but was not assigned any particular format.</p>
<pre><code class="language-python">df.Date.dtype
</code></pre>
<blockquote>
<pre><code>&gt;&gt; dtype('O')
</code></pre>
</blockquote>
<p>In order to have a standardized datetime, <code>pandas.to_datetime</code>can be applied to get a datetime, and then <code>tz_convert</code>to make it french time.</p>
<pre><code class="language-python">df['Date'] = pd.to_datetime(df.Date, utc=True).dt.tz_convert('Europe/Paris')
</code></pre>
<h4 id="3-filtering-dates">3. Filtering dates</h4>
<p>To know when data collection has actually been put into place, let's analyse dates contained in the DataFrame.</p>
<pre><code class="language-python">df.Date.dt.year.value_counts().sort_index()
</code></pre>
<table><thead><tr><th>Date</th><th>Count</th></tr></thead><tbody><tr><td>2013</td><td>15698</td></tr><tr><td>2014</td><td>102793</td></tr><tr><td>2015</td><td>111706</td></tr><tr><td>2016</td><td>107595</td></tr><tr><td>2017</td><td>107662</td></tr><tr><td>2018</td><td>116308</td></tr><tr><td>2019</td><td>122637</td></tr><tr><td>2020</td><td>26143</td></tr></tbody></table>
<p>To keep consistency, I decided to drop data from 2013: data collection may have not started everywhere, and had definitely not started in January 2013. If we want to dig deeper into the data and check correlations with passenger count, it would be difficult to achieve if data are not complete. Therefore only data collected from 2014 onwards will be kept:</p>
<pre><code class="language-python">df = df[df.Date.dt.year &gt;= 2014]
</code></pre>
<h4 id="5-grouping-data-by-year-and-month">5. Grouping data by year and month</h4>
<p>In the first chart, I wanted to plot found item count for each month in the dataset. To do so, we need to group by year, then month.</p>
<pre><code class="language-python">df_months = df.groupby(by=[df.Date.dt.year, df.Date.dt.month]).Date.count()
</code></pre>
<p>The result is the following:</p>
<pre><code class="language-python">2014  1       5788
      2       5815
      3       7012
      4       8543
      5       8657
              ... 
2020  2       9271
      3       4473
      4         72
      5       1386
      6       3649
</code></pre>
<p>Now we need to get the index back to datetime, combining years and months.</p>
<pre><code class="language-python">df_months.index = df_months.index.map(lambda t: pd.to_datetime('-'.join(map(str, t))))
</code></pre>
<h4 id="6-grouping-data-by-train-station">6. Grouping data by train station</h4>
<p>The second chart was made after grouping by train station. It can be achieved by the following instructions:</p>
<pre><code class="language-python">df_grouped = (df
    .groupby(by='Gare')
    .count()
    .Date
    .rename('Count')
    .sort_values(ascending=False)
)
</code></pre>
<p>This will first group by train station name, then apply a count aggregation, keep the Date renamed as Count, and values will be sorted from greatest to lowest count.</p>
<h4 id="7-pareto">7. Pareto</h4>
<p>The <code>df_grouped</code>DataFrame allows us to demonstrate an application of the Pareto priciple. The following lines are doing:</p>
<ul>
<li>Cumulative found item count of <code>df_grouped</code></li>
<li>Sum of all items in <code>df_grouped</code></li>
<li>Assigning booleans to cumulative counts below 80% of the total number of items, and counting them</li>
</ul>
<pre><code class="language-python">pareto = (df_grouped.cumsum() &lt; df_grouped.sum() * 0.8).sum()
</code></pre>
<p>The <code>df_grouped</code>DataFrame contains 181 train stations, and 32  (18%) of them account for 80% of the total count of found items.</p>
<h4 id="8-filtering-grouped-data">8. Filtering grouped data</h4>
<p>Using the Pareto principle described above, I filtered <code>df_grouped</code>to keep the first 32 rows for visualization purpose.</p>
<pre><code class="language-python">df_grouped = df_grouped.iloc[:pareto]
</code></pre>
<h2 id="visualization">Visualization</h2>
<p>Now that all the processing part is completed, we can proceed to visualization.</p>
<h3 id="1-monthly-found-items">1. Monthly found items</h3>
<p>The first chart is simply a bar chart of the total count of found items per month. <code>x</code> will therefore be the index of the dataframe <code>df_months</code>, and <code>y</code>  will be the sum of all found items found for the associated months.</p>
<pre><code class="language-python">fig = go.Figure()
fig.add_trace(
    go.Bar(
        x=df_months.index,
        y=df_months, 
        hoverinfo="x+y",
        marker_color="#007bff",  
))
</code></pre>
<p>Then, y axis title is added, as well as margins to enhance the result.</p>
<pre><code class="language-python">fig.update_layout(
    yaxis=dict(title="Found items in train stations", titlefont=dict(size=16)),
    margin={'l': 30, 'r': 30, 't': 50, 'b': 0},
)
</code></pre>
<h3 id="2-found-items-per-train-station">2. Found items per train station</h3>
<p>The second chart is also a bar chart. The data plotted are the counts of found items in each train station. As mentioned in the processing part, only the first 32 stations are displayed, ordered by number of found items.</p>
<pre><code class="language-python">fig.add_trace(
    go.Bar(
        x=np.arange(1, len(df_grouped)), 
        y=df_grouped, 
        text=df_grouped.index,
        meta = df_grouped,
        hoverinfo="text+y",
        name="Found items",
        marker_color="#007bff"
))
</code></pre>
<p>The final step is adjusting the layout: margins, axis titles and log scale for the y axis.</p>
<pre><code class="language-python">
fig.update_layout(
    margin={'l':20, 'r': 0, 't': 0, 'b': 250},
    xaxis=dict(tickangle=-90,
               tickfont={'size': 14}),
    yaxis=dict(title="Found items per station",
               type="log",
               titlefont=dict(size=16))
)
</code></pre>
<p>Link to the <a href="https://github.com/hugolmn/dataviz/blob/master/SNCF_found_item_1.ipynb">Jupyter notebook</a>.</p>]]></content:encoded>
            <category>Data Visualization</category>
            <category>Python</category>
            <category>plotly</category>
        </item>
        <item>
            <title><![CDATA[SNCF Open Data: train station attendance]]></title>
            <link>https://hugolmn.github.io/blog/2020/06/17/sncf-train-station-attendance</link>
            <guid>https://hugolmn.github.io/blog/2020/06/17/sncf-train-station-attendance</guid>
            <pubDate>Wed, 17 Jun 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Geo-visualization of train stations]]></description>
            <content:encoded><![CDATA[<h2 id="passengers-per-french-train-station-in-2018">Passengers per french train station in 2018</h2>
<div class="plotly-container"><iframe width="100%" height="700" frameborder="0" scrolling="no" src="/graphs/sncf-train-stations.html"></iframe></div>
<h2 id="data">Data</h2>
<ul>
<li><a href="https://ressources.data.sncf.com/explore/dataset/frequentation-gares">SNCF : train station attendance</a></li>
<li><a href="https://ressources.data.sncf.com/explore/dataset/referentiel-gares-voyageurs">SNCF : train station list</a></li>
</ul>
<h2 id="libraries">Libraries</h2>
<p>The following libraries are imported:</p>
<ul>
<li><a href="https://pandas.pydata.org/">pandas</a> and <a href="https://numpy.org/">numpy</a> for data processing</li>
<li><a href="https://plotly.com/python/">plotly</a>.colors to use a specific color scale</li>
<li><a href="https://plotly.com/python/">plotly</a>.graph_object for data visualization</li>
</ul>
<pre><code class="language-python">import pandas as pd						
import numpy as np						
import plotly.colors
import plotly.graph_objects as go
</code></pre>
<h2 id="processing">Processing</h2>
<h4 id="1-reading-csv-files">1. Reading csv files</h4>
<pre><code class="language-python">df_frequentation = pd.read_csv('data/frequentation-gares.csv', sep=';')
df_gares = pd.read_csv('data/referentiel-gares-voyageurs.csv', sep=';')
</code></pre>
<p>Sample data from df_frequentation</p>
<table><thead><tr><th></th><th>Nom de la gare</th><th>Code UIC complet</th><th>Code postal</th><th>Segmentation DRG 2018</th><th>Total Voyageurs 2018</th><th>Total Voyageurs + Non voyageurs 2018</th><th>Total Voyageurs 2017</th><th>Total Voyageurs + Non voyageurs 2017</th><th>Total Voyageurs 2016</th><th>Total Voyageurs + Non voyageurs 2016</th><th>Total Voyageurs 2015</th><th>Total Voyageurs + Non voyageurs 2015</th></tr></thead><tbody><tr><td>0</td><td>Abancourt</td><td>87313759</td><td>60220</td><td>c</td><td>40228</td><td>40228</td><td>43760</td><td>43760</td><td>41096</td><td>41096.551614</td><td>39720</td><td>39720</td></tr><tr><td>1</td><td>Agay</td><td>87757559</td><td>83530</td><td>c</td><td>15093</td><td>15093</td><td>14154</td><td>14154</td><td>19240</td><td>19240.514370</td><td>19121</td><td>19121</td></tr><tr><td>2</td><td>Agde</td><td>87781278</td><td>34300</td><td>a</td><td>588297</td><td>735372</td><td>697091</td><td>871364</td><td>660656</td><td>825820.929253</td><td>662516</td><td>828146</td></tr><tr><td>3</td><td>Agonac</td><td>87595157</td><td>24460</td><td>c</td><td>1492</td><td>1492</td><td>1583</td><td>1583</td><td>1134</td><td>1134.699996</td><td>1127</td><td>1127</td></tr><tr><td>4</td><td>Aigrefeuille Le Thou</td><td>87485193</td><td>17290</td><td>c</td><td>18670</td><td>18670</td><td>14513</td><td>14513</td><td>266</td><td>266.157144</td><td>0</td><td>0</td></tr></tbody></table>
<p>Sample data from df_gares</p>
<table><thead><tr><th></th><th>Code plate-forme</th><th>Intitulé gare</th><th>Intitulé fronton de gare</th><th>Gare DRG</th><th>Gare étrangère</th><th>Agence gare</th><th>Région SNCF</th><th>Unité gare</th><th>UT</th><th>Nbre plateformes</th><th>...</th><th>Longitude WGS84</th><th>Latitude WGS84</th><th>Code UIC</th><th>TVS</th><th>Segment DRG</th><th>Niveau de service</th><th>SOP</th><th>RG</th><th>Date fin validité plateforme</th><th>WGS 84</th></tr></thead><tbody><tr><td>0</td><td>00007-1</td><td>Bourg-Madame</td><td>Bourg-Madame</td><td>True</td><td>False</td><td>Agence Grand Sud</td><td>REGION LANGUEDOC-ROUSSILLON</td><td>UG Languedoc Roussillon</td><td>BOURG MADAME GARE</td><td>1</td><td>...</td><td>1.948670</td><td>42.432407</td><td>87784876</td><td>BMD</td><td>c</td><td>1.0</td><td>NaN</td><td>GARES C LANGUEDOC ROUSSILLON</td><td>NaN</td><td>42.4324069,1.9486704</td></tr><tr><td>1</td><td>00014-1</td><td>Bolquère - Eyne</td><td>Bolquère - Eyne</td><td>True</td><td>False</td><td>Agence Grand Sud</td><td>REGION LANGUEDOC-ROUSSILLON</td><td>UG Languedoc Roussillon</td><td>BOLQUERE EYNE GARE</td><td>1</td><td>...</td><td>2.087559</td><td>42.497873</td><td>87784801</td><td>BQE</td><td>c</td><td>1.0</td><td>NaN</td><td>GARES C LANGUEDOC ROUSSILLON</td><td>NaN</td><td>42.4978734,2.0875591</td></tr><tr><td>2</td><td>00015-1</td><td>Mont-Louis - La Cabanasse</td><td>Mont-Louis - La Cabanasse</td><td>True</td><td>False</td><td>Agence Grand Sud</td><td>REGION LANGUEDOC-ROUSSILLON</td><td>UG Languedoc Roussillon</td><td>MONT LOUIS LA CABANASSE GARE</td><td>1</td><td>...</td><td>2.113138</td><td>42.502090</td><td>87784793</td><td>MTC</td><td>c</td><td>1.0</td><td>NaN</td><td>GARES C LANGUEDOC ROUSSILLON</td><td>NaN</td><td>42.5020902,2.1131379</td></tr><tr><td>3</td><td>00020-1</td><td>Thuès les Bains</td><td>Thuès les Bains</td><td>True</td><td>False</td><td>Agence Grand Sud</td><td>REGION LANGUEDOC-ROUSSILLON</td><td>UG Languedoc Roussillon</td><td>THUES LES BAINS GARE</td><td>1</td><td>...</td><td>2.249094</td><td>42.528801</td><td>87784744</td><td>THB</td><td>c</td><td>1.0</td><td>NaN</td><td>GARES C LANGUEDOC ROUSSILLON</td><td>NaN</td><td>42.5288009,2.249094</td></tr></tbody></table>
<h4 id="2-merging-dataframes">2. Merging dataframes</h4>
<p>The UIC Code is a unique ID for train stations. However, the column names are different in both files, so it's mandatory so specify the <strong><code>left_on</code></strong> and <strong><code>right_on</code></strong> arguments.</p>
<pre><code class="language-python">df = df_gares.merge(
    right=df_frequentation,
    left_on='Code UIC',
    right_on='Code UIC complet',
    how='inner')
</code></pre>
<h4 id="3-filtering">3. Filtering</h4>
<p>In order to avoid keeping small train stations, I chose to filter out stations with attendance below 1000 passengers in 2018. For visualization purpose, I added a column holding the square root of the number of passengers per station</p>
<pre><code class="language-python">df = df[df['Total Voyageurs 2018'] &gt; 1000]
</code></pre>
<h4 id="4-adding-a-category-column">4. Adding a category column</h4>
<p>By using <code>pandas.cut</code> data can be split into categories according to total number of passengers. This will allow to plot with a different color for each category.</p>
<pre><code class="language-python">df['category'] = pd.cut(df['Total Voyageurs 2018'], bins=[1e4, 1e5, 1e6, 1e7, np.inf])
</code></pre>
<h2 id="visualization">Visualization</h2>
<p><a href="https://plotly.com/">Plotly</a> is a handy tool when it comes to creating interactive graphs and plots, that you can embed in other websites.</p>
<h4 id="1-scatter-mapbox">1. Scatter Mapbox</h4>
<p>Data contain latitude and longitude: these will be used to plot train stations on the map. The size of the bubbles will depend on the square root of the number of passengers in 2018. A different trace is added for each of the categories defined above. Finally, information shown on mouse-hovering is defined using <code>hovertemplate</code>.</p>
<pre><code class="language-python">fig = go.Figure()
colors = plotly.colors.sequential.Viridis

for i, cat in enumerate(df.category.cat.categories):
    df_sub = df[df.category == cat]
    fig.add_trace(go.Scattermapbox(
        lat=df_sub['Latitude WGS84'], 
        lon=df_sub['Longitude WGS84'],
        text=df_sub['Intitulé gare'],
        marker=dict(
            color=colors[2*i+1],
            size=np.sqrt(df_sub['Total Voyageurs 2018 sqrt']),
            sizemin=1,
            sizeref=15,
            sizemode='area',
            opacity=.8,
        ),
        meta=df_sub['Total Voyageurs 2018'],
        hovertemplate="%{text}" + "&lt;br&gt;" + "Passengers: %{meta}",
        name=f'&gt; {cat.left:1.0e} passengers',          
))
</code></pre>
<h4 id="2-layout">2. Layout</h4>
<p>The last step is adding the background map, the title, margins around the plot, and the initial position &amp; zoom.</p>
<pre><code class="language-python">fig.update_layout(
    mapbox_style="open-street-map",
    title='Passengers per french train station in 2018',
    margin={'l': 0, 'r': 0, 't': 50, 'b': 0},
    mapbox=dict(
        center={'lon': 2.39, 'lat': 47.09},
        zoom=4
    ),
)
</code></pre>
<p>Link to the <a href="https://github.com/hugolmn/dataviz/blob/master/SNCF_traffic.ipynb">Jupyter notebook</a>.</p>]]></content:encoded>
            <category>Data Visualization</category>
            <category>Python</category>
            <category>plotly</category>
        </item>
    </channel>
</rss>